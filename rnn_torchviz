digraph {
	graph [size="184.04999999999998,184.04999999999998"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2159155052928 [label="
 (96, 10)" fillcolor=darkolivegreen1]
	2159155005280 [label=AddmmBackward0]
	2159155005664 -> 2159155005280
	2159143386336 [label="fc.bias
 (10)" fillcolor=lightblue]
	2159143386336 -> 2159155005664
	2159155005664 [label=AccumulateGrad]
	2159155005568 -> 2159155005280
	2159155005568 [label=ReshapeAliasBackward0]
	2159155005520 -> 2159155005568
	2159155005520 [label=MulBackward0]
	2159155005808 -> 2159155005520
	2159155005808 [label=AvgPool2DBackward0]
	2159155005904 -> 2159155005808
	2159155005904 [label=ReluBackward0]
	2159155006000 -> 2159155005904
	2159155006000 [label=AddBackward0]
	2159155006096 -> 2159155006000
	2159155006096 [label=NativeBatchNormBackward0]
	2159155006240 -> 2159155006096
	2159155006240 [label=ConvolutionBackward0]
	2159155006432 -> 2159155006240
	2159155006432 [label=ReluBackward0]
	2159155006624 -> 2159155006432
	2159155006624 [label=NativeBatchNormBackward0]
	2159155006720 -> 2159155006624
	2159155006720 [label=ConvolutionBackward0]
	2159155006912 -> 2159155006720
	2159155006912 [label=ReluBackward0]
	2159155007104 -> 2159155006912
	2159155007104 [label=NativeBatchNormBackward0]
	2159155007200 -> 2159155007104
	2159155007200 [label=ConvolutionBackward0]
	2159155006048 -> 2159155007200
	2159155006048 [label=ReluBackward0]
	2159155007536 -> 2159155006048
	2159155007536 [label=AddBackward0]
	2159155007632 -> 2159155007536
	2159155007632 [label=NativeBatchNormBackward0]
	2159155007776 -> 2159155007632
	2159155007776 [label=ConvolutionBackward0]
	2159155007968 -> 2159155007776
	2159155007968 [label=ReluBackward0]
	2159155008160 -> 2159155007968
	2159155008160 [label=NativeBatchNormBackward0]
	2159155008256 -> 2159155008160
	2159155008256 [label=ConvolutionBackward0]
	2159155008448 -> 2159155008256
	2159155008448 [label=ReluBackward0]
	2159155008640 -> 2159155008448
	2159155008640 [label=NativeBatchNormBackward0]
	2159155008688 -> 2159155008640
	2159155008688 [label=ConvolutionBackward0]
	2159155007584 -> 2159155008688
	2159155007584 [label=ReluBackward0]
	2159155009120 -> 2159155007584
	2159155009120 [label=AddBackward0]
	2159155009168 -> 2159155009120
	2159155009168 [label=NativeBatchNormBackward0]
	2159155009408 -> 2159155009168
	2159155009408 [label=ConvolutionBackward0]
	2159155009600 -> 2159155009408
	2159155009600 [label=ReluBackward0]
	2159155009792 -> 2159155009600
	2159155009792 [label=NativeBatchNormBackward0]
	2159155009840 -> 2159155009792
	2159155009840 [label=ConvolutionBackward0]
	2159155010128 -> 2159155009840
	2159155010128 [label=ReluBackward0]
	2159155010320 -> 2159155010128
	2159155010320 [label=NativeBatchNormBackward0]
	2159155010368 -> 2159155010320
	2159155010368 [label=ConvolutionBackward0]
	2159155010656 -> 2159155010368
	2159155010656 [label=ReluBackward0]
	2159155010848 -> 2159155010656
	2159155010848 [label=AddBackward0]
	2159155010896 -> 2159155010848
	2159155010896 [label=NativeBatchNormBackward0]
	2159155011136 -> 2159155010896
	2159155011136 [label=ConvolutionBackward0]
	2159155011328 -> 2159155011136
	2159155011328 [label=ReluBackward0]
	2159155011520 -> 2159155011328
	2159155011520 [label=NativeBatchNormBackward0]
	2159155011568 -> 2159155011520
	2159155011568 [label=ConvolutionBackward0]
	2159155011856 -> 2159155011568
	2159155011856 [label=ReluBackward0]
	2159155012048 -> 2159155011856
	2159155012048 [label=NativeBatchNormBackward0]
	2159155012096 -> 2159155012048
	2159155012096 [label=ConvolutionBackward0]
	2159155010752 -> 2159155012096
	2159155010752 [label=ReluBackward0]
	2159155012528 -> 2159155010752
	2159155012528 [label=AddBackward0]
	2159155012576 -> 2159155012528
	2159155012576 [label=NativeBatchNormBackward0]
	2159155012816 -> 2159155012576
	2159155012816 [label=ConvolutionBackward0]
	2159155013008 -> 2159155012816
	2159155013008 [label=ReluBackward0]
	2159155013200 -> 2159155013008
	2159155013200 [label=NativeBatchNormBackward0]
	2159155013248 -> 2159155013200
	2159155013248 [label=ConvolutionBackward0]
	2159155013536 -> 2159155013248
	2159155013536 [label=ReluBackward0]
	2159155013728 -> 2159155013536
	2159155013728 [label=NativeBatchNormBackward0]
	2159155013776 -> 2159155013728
	2159155013776 [label=ConvolutionBackward0]
	2159155012432 -> 2159155013776
	2159155012432 [label=ReluBackward0]
	2159155014160 -> 2159155012432
	2159155014160 [label=AddBackward0]
	2159155014208 -> 2159155014160
	2159155014208 [label=NativeBatchNormBackward0]
	2159155014448 -> 2159155014208
	2159155014448 [label=ConvolutionBackward0]
	2159155014640 -> 2159155014448
	2159155014640 [label=ReluBackward0]
	2159155014832 -> 2159155014640
	2159155014832 [label=NativeBatchNormBackward0]
	2159155014880 -> 2159155014832
	2159155014880 [label=ConvolutionBackward0]
	2159155015168 -> 2159155014880
	2159155015168 [label=ReluBackward0]
	2159155015360 -> 2159155015168
	2159155015360 [label=NativeBatchNormBackward0]
	2159155015408 -> 2159155015360
	2159155015408 [label=ConvolutionBackward0]
	2159155014064 -> 2159155015408
	2159155014064 [label=ReluBackward0]
	2159155015840 -> 2159155014064
	2159155015840 [label=AddBackward0]
	2159155015888 -> 2159155015840
	2159155015888 [label=NativeBatchNormBackward0]
	2159155016128 -> 2159155015888
	2159155016128 [label=ConvolutionBackward0]
	2159155016320 -> 2159155016128
	2159155016320 [label=ReluBackward0]
	2159155016512 -> 2159155016320
	2159155016512 [label=NativeBatchNormBackward0]
	2159155016560 -> 2159155016512
	2159155016560 [label=ConvolutionBackward0]
	2159155016848 -> 2159155016560
	2159155016848 [label=ReluBackward0]
	2159155017040 -> 2159155016848
	2159155017040 [label=NativeBatchNormBackward0]
	2159155017088 -> 2159155017040
	2159155017088 [label=ConvolutionBackward0]
	2159155015744 -> 2159155017088
	2159155015744 [label=ReluBackward0]
	2159155017520 -> 2159155015744
	2159155017520 [label=AddBackward0]
	2159155017568 -> 2159155017520
	2159155017568 [label=NativeBatchNormBackward0]
	2159155017808 -> 2159155017568
	2159155017808 [label=ConvolutionBackward0]
	2159155018000 -> 2159155017808
	2159155018000 [label=ReluBackward0]
	2159155018192 -> 2159155018000
	2159155018192 [label=NativeBatchNormBackward0]
	2159155018240 -> 2159155018192
	2159155018240 [label=ConvolutionBackward0]
	2159155018528 -> 2159155018240
	2159155018528 [label=ReluBackward0]
	2159155018720 -> 2159155018528
	2159155018720 [label=NativeBatchNormBackward0]
	2159155018768 -> 2159155018720
	2159155018768 [label=ConvolutionBackward0]
	2159155017424 -> 2159155018768
	2159155017424 [label=ReluBackward0]
	2159155019200 -> 2159155017424
	2159155019200 [label=AddBackward0]
	2159155019248 -> 2159155019200
	2159155019248 [label=NativeBatchNormBackward0]
	2159155019488 -> 2159155019248
	2159155019488 [label=ConvolutionBackward0]
	2159155019680 -> 2159155019488
	2159155019680 [label=ReluBackward0]
	2159143698592 -> 2159155019680
	2159143698592 [label=NativeBatchNormBackward0]
	2159143698640 -> 2159143698592
	2159143698640 [label=ConvolutionBackward0]
	2159143698928 -> 2159143698640
	2159143698928 [label=ReluBackward0]
	2159143699120 -> 2159143698928
	2159143699120 [label=NativeBatchNormBackward0]
	2159143699168 -> 2159143699120
	2159143699168 [label=ConvolutionBackward0]
	2159143699456 -> 2159143699168
	2159143699456 [label=ReluBackward0]
	2159143699648 -> 2159143699456
	2159143699648 [label=AddBackward0]
	2159143699696 -> 2159143699648
	2159143699696 [label=NativeBatchNormBackward0]
	2159143699936 -> 2159143699696
	2159143699936 [label=ConvolutionBackward0]
	2159143700128 -> 2159143699936
	2159143700128 [label=ReluBackward0]
	2159143700320 -> 2159143700128
	2159143700320 [label=NativeBatchNormBackward0]
	2159143700368 -> 2159143700320
	2159143700368 [label=ConvolutionBackward0]
	2159143700656 -> 2159143700368
	2159143700656 [label=ReluBackward0]
	2159143700848 -> 2159143700656
	2159143700848 [label=NativeBatchNormBackward0]
	2159143700896 -> 2159143700848
	2159143700896 [label=ConvolutionBackward0]
	2159143699552 -> 2159143700896
	2159143699552 [label=ReluBackward0]
	2159143701328 -> 2159143699552
	2159143701328 [label=AddBackward0]
	2159143701376 -> 2159143701328
	2159143701376 [label=NativeBatchNormBackward0]
	2159143701616 -> 2159143701376
	2159143701616 [label=ConvolutionBackward0]
	2159143701808 -> 2159143701616
	2159143701808 [label=ReluBackward0]
	2159143702000 -> 2159143701808
	2159143702000 [label=NativeBatchNormBackward0]
	2159143702048 -> 2159143702000
	2159143702048 [label=ConvolutionBackward0]
	2159143702336 -> 2159143702048
	2159143702336 [label=ReluBackward0]
	2159143702528 -> 2159143702336
	2159143702528 [label=NativeBatchNormBackward0]
	2159143702576 -> 2159143702528
	2159143702576 [label=ConvolutionBackward0]
	2159143701232 -> 2159143702576
	2159143701232 [label=ReluBackward0]
	2159143703008 -> 2159143701232
	2159143703008 [label=AddBackward0]
	2159143703056 -> 2159143703008
	2159143703056 [label=NativeBatchNormBackward0]
	2159143703296 -> 2159143703056
	2159143703296 [label=ConvolutionBackward0]
	2159143703488 -> 2159143703296
	2159143703488 [label=ReluBackward0]
	2159143703680 -> 2159143703488
	2159143703680 [label=NativeBatchNormBackward0]
	2159143703728 -> 2159143703680
	2159143703728 [label=ConvolutionBackward0]
	2159143704016 -> 2159143703728
	2159143704016 [label=ReluBackward0]
	2159143704208 -> 2159143704016
	2159143704208 [label=NativeBatchNormBackward0]
	2159143704256 -> 2159143704208
	2159143704256 [label=ConvolutionBackward0]
	2159143702912 -> 2159143704256
	2159143702912 [label=ReluBackward0]
	2159143704688 -> 2159143702912
	2159143704688 [label=AddBackward0]
	2159143704736 -> 2159143704688
	2159143704736 [label=NativeBatchNormBackward0]
	2159143704976 -> 2159143704736
	2159143704976 [label=ConvolutionBackward0]
	2159143705168 -> 2159143704976
	2159143705168 [label=ReluBackward0]
	2159143705360 -> 2159143705168
	2159143705360 [label=NativeBatchNormBackward0]
	2159143705408 -> 2159143705360
	2159143705408 [label=ConvolutionBackward0]
	2159143705696 -> 2159143705408
	2159143705696 [label=ReluBackward0]
	2159143705888 -> 2159143705696
	2159143705888 [label=NativeBatchNormBackward0]
	2159143705936 -> 2159143705888
	2159143705936 [label=ConvolutionBackward0]
	2159143706224 -> 2159143705936
	2159143706224 [label=ReluBackward0]
	2159143706416 -> 2159143706224
	2159143706416 [label=AddBackward0]
	2159143706464 -> 2159143706416
	2159143706464 [label=NativeBatchNormBackward0]
	2159143706704 -> 2159143706464
	2159143706704 [label=ConvolutionBackward0]
	2159143706896 -> 2159143706704
	2159143706896 [label=ReluBackward0]
	2159143707088 -> 2159143706896
	2159143707088 [label=NativeBatchNormBackward0]
	2159143707136 -> 2159143707088
	2159143707136 [label=ConvolutionBackward0]
	2159143707424 -> 2159143707136
	2159143707424 [label=ReluBackward0]
	2159143707616 -> 2159143707424
	2159143707616 [label=NativeBatchNormBackward0]
	2159143707664 -> 2159143707616
	2159143707664 [label=ConvolutionBackward0]
	2159143706320 -> 2159143707664
	2159143706320 [label=ReluBackward0]
	2159143708096 -> 2159143706320
	2159143708096 [label=AddBackward0]
	2159143708144 -> 2159143708096
	2159143708144 [label=NativeBatchNormBackward0]
	2159143708384 -> 2159143708144
	2159143708384 [label=ConvolutionBackward0]
	2159143708576 -> 2159143708384
	2159143708576 [label=ReluBackward0]
	2159143708768 -> 2159143708576
	2159143708768 [label=NativeBatchNormBackward0]
	2159143708816 -> 2159143708768
	2159143708816 [label=ConvolutionBackward0]
	2159143709104 -> 2159143708816
	2159143709104 [label=ReluBackward0]
	2159143709296 -> 2159143709104
	2159143709296 [label=NativeBatchNormBackward0]
	2159143709344 -> 2159143709296
	2159143709344 [label=ConvolutionBackward0]
	2159143708000 -> 2159143709344
	2159143708000 [label=ReluBackward0]
	2159143709776 -> 2159143708000
	2159143709776 [label=AddBackward0]
	2159143709824 -> 2159143709776
	2159143709824 [label=NativeBatchNormBackward0]
	2159143710064 -> 2159143709824
	2159143710064 [label=ConvolutionBackward0]
	2159143710256 -> 2159143710064
	2159143710256 [label=ReluBackward0]
	2159143710448 -> 2159143710256
	2159143710448 [label=NativeBatchNormBackward0]
	2159143710496 -> 2159143710448
	2159143710496 [label=ConvolutionBackward0]
	2159143710784 -> 2159143710496
	2159143710784 [label=ReluBackward0]
	2159143710976 -> 2159143710784
	2159143710976 [label=NativeBatchNormBackward0]
	2159143711024 -> 2159143710976
	2159143711024 [label=ConvolutionBackward0]
	2159143711312 -> 2159143711024
	2159143711312 [label=MaxPool2DWithIndicesBackward0]
	2159143711504 -> 2159143711312
	2159143711504 [label=ReluBackward0]
	2159143711552 -> 2159143711504
	2159143711552 [label=NativeBatchNormBackward0]
	2159143711696 -> 2159143711552
	2159143711696 [label=ConvolutionBackward0]
	2159143711984 -> 2159143711696
	2159074531088 [label="conv1.0.weight
 (16, 1, 7, 7)" fillcolor=lightblue]
	2159074531088 -> 2159143711984
	2159143711984 [label=AccumulateGrad]
	2159143711936 -> 2159143711696
	2159074537488 [label="conv1.0.bias
 (16)" fillcolor=lightblue]
	2159074537488 -> 2159143711936
	2159143711936 [label=AccumulateGrad]
	2159143711648 -> 2159143711552
	2159074537408 [label="conv1.1.weight
 (16)" fillcolor=lightblue]
	2159074537408 -> 2159143711648
	2159143711648 [label=AccumulateGrad]
	2159143711792 -> 2159143711552
	2159074531008 [label="conv1.1.bias
 (16)" fillcolor=lightblue]
	2159074531008 -> 2159143711792
	2159143711792 [label=AccumulateGrad]
	2159143711264 -> 2159143711024
	2159074536848 [label="block0.0.conv1.0.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	2159074536848 -> 2159143711264
	2159143711264 [label=AccumulateGrad]
	2159143711216 -> 2159143711024
	2159074536768 [label="block0.0.conv1.0.bias
 (16)" fillcolor=lightblue]
	2159074536768 -> 2159143711216
	2159143711216 [label=AccumulateGrad]
	2159143710880 -> 2159143710976
	2159074530368 [label="block0.0.conv1.1.weight
 (16)" fillcolor=lightblue]
	2159074530368 -> 2159143710880
	2159143710880 [label=AccumulateGrad]
	2159143711120 -> 2159143710976
	2159074530288 [label="block0.0.conv1.1.bias
 (16)" fillcolor=lightblue]
	2159074530288 -> 2159143711120
	2159143711120 [label=AccumulateGrad]
	2159143710736 -> 2159143710496
	2159074536448 [label="block0.0.conv2.0.weight
 (16, 16, 7, 7)" fillcolor=lightblue]
	2159074536448 -> 2159143710736
	2159143710736 [label=AccumulateGrad]
	2159143710688 -> 2159143710496
	2159074530048 [label="block0.0.conv2.0.bias
 (16)" fillcolor=lightblue]
	2159074530048 -> 2159143710688
	2159143710688 [label=AccumulateGrad]
	2159143710352 -> 2159143710448
	2159074529968 [label="block0.0.conv2.1.weight
 (16)" fillcolor=lightblue]
	2159074529968 -> 2159143710352
	2159143710352 [label=AccumulateGrad]
	2159143710592 -> 2159143710448
	2159074536368 [label="block0.0.conv2.1.bias
 (16)" fillcolor=lightblue]
	2159074536368 -> 2159143710592
	2159143710592 [label=AccumulateGrad]
	2159143710208 -> 2159143710064
	2159074529728 [label="block0.0.conv3.0.weight
 (64, 16, 3, 3)" fillcolor=lightblue]
	2159074529728 -> 2159143710208
	2159143710208 [label=AccumulateGrad]
	2159143710160 -> 2159143710064
	2159074529648 [label="block0.0.conv3.0.bias
 (64)" fillcolor=lightblue]
	2159074529648 -> 2159143710160
	2159143710160 [label=AccumulateGrad]
	2159143710016 -> 2159143709824
	2159074535888 [label="block0.0.conv3.1.weight
 (64)" fillcolor=lightblue]
	2159074535888 -> 2159143710016
	2159143710016 [label=AccumulateGrad]
	2159143709968 -> 2159143709824
	2159074535808 [label="block0.0.conv3.1.bias
 (64)" fillcolor=lightblue]
	2159074535808 -> 2159143709968
	2159143709968 [label=AccumulateGrad]
	2159143709680 -> 2159143709776
	2159143709680 [label=NativeBatchNormBackward0]
	2159143710400 -> 2159143709680
	2159143710400 [label=ConvolutionBackward0]
	2159143711312 -> 2159143710400
	2159143711168 -> 2159143710400
	2159074530768 [label="block0.0.downsample.0.weight
 (64, 16, 3, 3)" fillcolor=lightblue]
	2159074530768 -> 2159143711168
	2159143711168 [label=AccumulateGrad]
	2159143710928 -> 2159143710400
	2159074537168 [label="block0.0.downsample.0.bias
 (64)" fillcolor=lightblue]
	2159074537168 -> 2159143710928
	2159143710928 [label=AccumulateGrad]
	2159143710304 -> 2159143709680
	2159074537088 [label="block0.0.downsample.1.weight
 (64)" fillcolor=lightblue]
	2159074537088 -> 2159143710304
	2159143710304 [label=AccumulateGrad]
	2159143710112 -> 2159143709680
	2159074530688 [label="block0.0.downsample.1.bias
 (64)" fillcolor=lightblue]
	2159074530688 -> 2159143710112
	2159143710112 [label=AccumulateGrad]
	2159143709632 -> 2159143709344
	2159074529568 [label="block0.1.conv1.0.weight
 (16, 64, 3, 3)" fillcolor=lightblue]
	2159074529568 -> 2159143709632
	2159143709632 [label=AccumulateGrad]
	2159143709584 -> 2159143709344
	2159074529488 [label="block0.1.conv1.0.bias
 (16)" fillcolor=lightblue]
	2159074529488 -> 2159143709584
	2159143709584 [label=AccumulateGrad]
	2159143709200 -> 2159143709296
	2159074535728 [label="block0.1.conv1.1.weight
 (16)" fillcolor=lightblue]
	2159074535728 -> 2159143709200
	2159143709200 [label=AccumulateGrad]
	2159143709440 -> 2159143709296
	2159074535648 [label="block0.1.conv1.1.bias
 (16)" fillcolor=lightblue]
	2159074535648 -> 2159143709440
	2159143709440 [label=AccumulateGrad]
	2159143709056 -> 2159143708816
	2159074529008 [label="block0.1.conv2.0.weight
 (16, 16, 7, 7)" fillcolor=lightblue]
	2159074529008 -> 2159143709056
	2159143709056 [label=AccumulateGrad]
	2159143709008 -> 2159143708816
	2159074535408 [label="block0.1.conv2.0.bias
 (16)" fillcolor=lightblue]
	2159074535408 -> 2159143709008
	2159143709008 [label=AccumulateGrad]
	2159143708672 -> 2159143708768
	2159074535328 [label="block0.1.conv2.1.weight
 (16)" fillcolor=lightblue]
	2159074535328 -> 2159143708672
	2159143708672 [label=AccumulateGrad]
	2159143708912 -> 2159143708768
	2159074528928 [label="block0.1.conv2.1.bias
 (16)" fillcolor=lightblue]
	2159074528928 -> 2159143708912
	2159143708912 [label=AccumulateGrad]
	2159143708528 -> 2159143708384
	2159074528688 [label="block0.1.conv3.0.weight
 (64, 16, 3, 3)" fillcolor=lightblue]
	2159074528688 -> 2159143708528
	2159143708528 [label=AccumulateGrad]
	2159143708480 -> 2159143708384
	2159074534928 [label="block0.1.conv3.0.bias
 (64)" fillcolor=lightblue]
	2159074534928 -> 2159143708480
	2159143708480 [label=AccumulateGrad]
	2159143708336 -> 2159143708144
	2159074534848 [label="block0.1.conv3.1.weight
 (64)" fillcolor=lightblue]
	2159074534848 -> 2159143708336
	2159143708336 [label=AccumulateGrad]
	2159143708288 -> 2159143708144
	2159074528448 [label="block0.1.conv3.1.bias
 (64)" fillcolor=lightblue]
	2159074528448 -> 2159143708288
	2159143708288 [label=AccumulateGrad]
	2159143708000 -> 2159143708096
	2159143707952 -> 2159143707664
	2159074534608 [label="block0.2.conv1.0.weight
 (16, 64, 3, 3)" fillcolor=lightblue]
	2159074534608 -> 2159143707952
	2159143707952 [label=AccumulateGrad]
	2159143707904 -> 2159143707664
	2159074534528 [label="block0.2.conv1.0.bias
 (16)" fillcolor=lightblue]
	2159074534528 -> 2159143707904
	2159143707904 [label=AccumulateGrad]
	2159143707520 -> 2159143707616
	2159074541488 [label="block0.2.conv1.1.weight
 (16)" fillcolor=lightblue]
	2159074541488 -> 2159143707520
	2159143707520 [label=AccumulateGrad]
	2159143707760 -> 2159143707616
	2159074528128 [label="block0.2.conv1.1.bias
 (16)" fillcolor=lightblue]
	2159074528128 -> 2159143707760
	2159143707760 [label=AccumulateGrad]
	2159143707376 -> 2159143707136
	2159074527968 [label="block0.2.conv2.0.weight
 (16, 16, 7, 7)" fillcolor=lightblue]
	2159074527968 -> 2159143707376
	2159143707376 [label=AccumulateGrad]
	2159143707328 -> 2159143707136
	2159074527888 [label="block0.2.conv2.0.bias
 (16)" fillcolor=lightblue]
	2159074527888 -> 2159143707328
	2159143707328 [label=AccumulateGrad]
	2159143706992 -> 2159143707088
	2159074534288 [label="block0.2.conv2.1.weight
 (16)" fillcolor=lightblue]
	2159074534288 -> 2159143706992
	2159143706992 [label=AccumulateGrad]
	2159143707232 -> 2159143707088
	2159074534208 [label="block0.2.conv2.1.bias
 (16)" fillcolor=lightblue]
	2159074534208 -> 2159143707232
	2159143707232 [label=AccumulateGrad]
	2159143706848 -> 2159143706704
	2159074534048 [label="block0.2.conv3.0.weight
 (64, 16, 3, 3)" fillcolor=lightblue]
	2159074534048 -> 2159143706848
	2159143706848 [label=AccumulateGrad]
	2159143706800 -> 2159143706704
	2159074541088 [label="block0.2.conv3.0.bias
 (64)" fillcolor=lightblue]
	2159074541088 -> 2159143706800
	2159143706800 [label=AccumulateGrad]
	2159143706656 -> 2159143706464
	2159074541008 [label="block0.2.conv3.1.weight
 (64)" fillcolor=lightblue]
	2159074541008 -> 2159143706656
	2159143706656 [label=AccumulateGrad]
	2159143706608 -> 2159143706464
	2159074527648 [label="block0.2.conv3.1.bias
 (64)" fillcolor=lightblue]
	2159074527648 -> 2159143706608
	2159143706608 [label=AccumulateGrad]
	2159143706320 -> 2159143706416
	2159143706176 -> 2159143705936
	2159142882336 [label="block1.0.conv1.0.weight
 (32, 64, 3, 3)" fillcolor=lightblue]
	2159142882336 -> 2159143706176
	2159143706176 [label=AccumulateGrad]
	2159143706128 -> 2159143705936
	2159142882416 [label="block1.0.conv1.0.bias
 (32)" fillcolor=lightblue]
	2159142882416 -> 2159143706128
	2159143706128 [label=AccumulateGrad]
	2159143705792 -> 2159143705888
	2159142882496 [label="block1.0.conv1.1.weight
 (32)" fillcolor=lightblue]
	2159142882496 -> 2159143705792
	2159143705792 [label=AccumulateGrad]
	2159143706032 -> 2159143705888
	2159142882576 [label="block1.0.conv1.1.bias
 (32)" fillcolor=lightblue]
	2159142882576 -> 2159143706032
	2159143706032 [label=AccumulateGrad]
	2159143705648 -> 2159143705408
	2159142883056 [label="block1.0.conv2.0.weight
 (32, 32, 7, 7)" fillcolor=lightblue]
	2159142883056 -> 2159143705648
	2159143705648 [label=AccumulateGrad]
	2159143705600 -> 2159143705408
	2159142883136 [label="block1.0.conv2.0.bias
 (32)" fillcolor=lightblue]
	2159142883136 -> 2159143705600
	2159143705600 [label=AccumulateGrad]
	2159143705264 -> 2159143705360
	2159142883216 [label="block1.0.conv2.1.weight
 (32)" fillcolor=lightblue]
	2159142883216 -> 2159143705264
	2159143705264 [label=AccumulateGrad]
	2159143705504 -> 2159143705360
	2159142883296 [label="block1.0.conv2.1.bias
 (32)" fillcolor=lightblue]
	2159142883296 -> 2159143705504
	2159143705504 [label=AccumulateGrad]
	2159143705120 -> 2159143704976
	2159142883776 [label="block1.0.conv3.0.weight
 (128, 32, 3, 3)" fillcolor=lightblue]
	2159142883776 -> 2159143705120
	2159143705120 [label=AccumulateGrad]
	2159143705072 -> 2159143704976
	2159142883856 [label="block1.0.conv3.0.bias
 (128)" fillcolor=lightblue]
	2159142883856 -> 2159143705072
	2159143705072 [label=AccumulateGrad]
	2159143704928 -> 2159143704736
	2159142883936 [label="block1.0.conv3.1.weight
 (128)" fillcolor=lightblue]
	2159142883936 -> 2159143704928
	2159143704928 [label=AccumulateGrad]
	2159143704880 -> 2159143704736
	2159142884016 [label="block1.0.conv3.1.bias
 (128)" fillcolor=lightblue]
	2159142884016 -> 2159143704880
	2159143704880 [label=AccumulateGrad]
	2159143704592 -> 2159143704688
	2159143704592 [label=NativeBatchNormBackward0]
	2159143705312 -> 2159143704592
	2159143705312 [label=ConvolutionBackward0]
	2159143706224 -> 2159143705312
	2159143706080 -> 2159143705312
	2159074539568 [label="block1.0.downsample.0.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2159074539568 -> 2159143706080
	2159143706080 [label=AccumulateGrad]
	2159143705840 -> 2159143705312
	2159074539488 [label="block1.0.downsample.0.bias
 (128)" fillcolor=lightblue]
	2159074539488 -> 2159143705840
	2159143705840 [label=AccumulateGrad]
	2159143705216 -> 2159143704592
	2159074533168 [label="block1.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2159074533168 -> 2159143705216
	2159143705216 [label=AccumulateGrad]
	2159143705024 -> 2159143704592
	2159074533088 [label="block1.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2159074533088 -> 2159143705024
	2159143705024 [label=AccumulateGrad]
	2159143704544 -> 2159143704256
	2159142884496 [label="block1.1.conv1.0.weight
 (32, 128, 3, 3)" fillcolor=lightblue]
	2159142884496 -> 2159143704544
	2159143704544 [label=AccumulateGrad]
	2159143704496 -> 2159143704256
	2159142884576 [label="block1.1.conv1.0.bias
 (32)" fillcolor=lightblue]
	2159142884576 -> 2159143704496
	2159143704496 [label=AccumulateGrad]
	2159143704112 -> 2159143704208
	2159142884656 [label="block1.1.conv1.1.weight
 (32)" fillcolor=lightblue]
	2159142884656 -> 2159143704112
	2159143704112 [label=AccumulateGrad]
	2159143704352 -> 2159143704208
	2159142884736 [label="block1.1.conv1.1.bias
 (32)" fillcolor=lightblue]
	2159142884736 -> 2159143704352
	2159143704352 [label=AccumulateGrad]
	2159143703968 -> 2159143703728
	2159142885136 [label="block1.1.conv2.0.weight
 (32, 32, 7, 7)" fillcolor=lightblue]
	2159142885136 -> 2159143703968
	2159143703968 [label=AccumulateGrad]
	2159143703920 -> 2159143703728
	2159142885216 [label="block1.1.conv2.0.bias
 (32)" fillcolor=lightblue]
	2159142885216 -> 2159143703920
	2159143703920 [label=AccumulateGrad]
	2159143703584 -> 2159143703680
	2159142885296 [label="block1.1.conv2.1.weight
 (32)" fillcolor=lightblue]
	2159142885296 -> 2159143703584
	2159143703584 [label=AccumulateGrad]
	2159143703824 -> 2159143703680
	2159142885376 [label="block1.1.conv2.1.bias
 (32)" fillcolor=lightblue]
	2159142885376 -> 2159143703824
	2159143703824 [label=AccumulateGrad]
	2159143703440 -> 2159143703296
	2159142885856 [label="block1.1.conv3.0.weight
 (128, 32, 3, 3)" fillcolor=lightblue]
	2159142885856 -> 2159143703440
	2159143703440 [label=AccumulateGrad]
	2159143703392 -> 2159143703296
	2159142885936 [label="block1.1.conv3.0.bias
 (128)" fillcolor=lightblue]
	2159142885936 -> 2159143703392
	2159143703392 [label=AccumulateGrad]
	2159143703248 -> 2159143703056
	2159142886016 [label="block1.1.conv3.1.weight
 (128)" fillcolor=lightblue]
	2159142886016 -> 2159143703248
	2159143703248 [label=AccumulateGrad]
	2159143703200 -> 2159143703056
	2159142886096 [label="block1.1.conv3.1.bias
 (128)" fillcolor=lightblue]
	2159142886096 -> 2159143703200
	2159143703200 [label=AccumulateGrad]
	2159143702912 -> 2159143703008
	2159143702864 -> 2159143702576
	2159142886576 [label="block1.2.conv1.0.weight
 (32, 128, 3, 3)" fillcolor=lightblue]
	2159142886576 -> 2159143702864
	2159143702864 [label=AccumulateGrad]
	2159143702816 -> 2159143702576
	2159142886656 [label="block1.2.conv1.0.bias
 (32)" fillcolor=lightblue]
	2159142886656 -> 2159143702816
	2159143702816 [label=AccumulateGrad]
	2159143702432 -> 2159143702528
	2159142886736 [label="block1.2.conv1.1.weight
 (32)" fillcolor=lightblue]
	2159142886736 -> 2159143702432
	2159143702432 [label=AccumulateGrad]
	2159143702672 -> 2159143702528
	2159142886816 [label="block1.2.conv1.1.bias
 (32)" fillcolor=lightblue]
	2159142886816 -> 2159143702672
	2159143702672 [label=AccumulateGrad]
	2159143702288 -> 2159143702048
	2159142887296 [label="block1.2.conv2.0.weight
 (32, 32, 7, 7)" fillcolor=lightblue]
	2159142887296 -> 2159143702288
	2159143702288 [label=AccumulateGrad]
	2159143702240 -> 2159143702048
	2159142887376 [label="block1.2.conv2.0.bias
 (32)" fillcolor=lightblue]
	2159142887376 -> 2159143702240
	2159143702240 [label=AccumulateGrad]
	2159143701904 -> 2159143702000
	2159142887456 [label="block1.2.conv2.1.weight
 (32)" fillcolor=lightblue]
	2159142887456 -> 2159143701904
	2159143701904 [label=AccumulateGrad]
	2159143702144 -> 2159143702000
	2159142887536 [label="block1.2.conv2.1.bias
 (32)" fillcolor=lightblue]
	2159142887536 -> 2159143702144
	2159143702144 [label=AccumulateGrad]
	2159143701760 -> 2159143701616
	2159142888016 [label="block1.2.conv3.0.weight
 (128, 32, 3, 3)" fillcolor=lightblue]
	2159142888016 -> 2159143701760
	2159143701760 [label=AccumulateGrad]
	2159143701712 -> 2159143701616
	2159142888096 [label="block1.2.conv3.0.bias
 (128)" fillcolor=lightblue]
	2159142888096 -> 2159143701712
	2159143701712 [label=AccumulateGrad]
	2159143701568 -> 2159143701376
	2159142888176 [label="block1.2.conv3.1.weight
 (128)" fillcolor=lightblue]
	2159142888176 -> 2159143701568
	2159143701568 [label=AccumulateGrad]
	2159143701520 -> 2159143701376
	2159142888256 [label="block1.2.conv3.1.bias
 (128)" fillcolor=lightblue]
	2159142888256 -> 2159143701520
	2159143701520 [label=AccumulateGrad]
	2159143701232 -> 2159143701328
	2159143701184 -> 2159143700896
	2159142888736 [label="block1.3.conv1.0.weight
 (32, 128, 3, 3)" fillcolor=lightblue]
	2159142888736 -> 2159143701184
	2159143701184 [label=AccumulateGrad]
	2159143701136 -> 2159143700896
	2159142888816 [label="block1.3.conv1.0.bias
 (32)" fillcolor=lightblue]
	2159142888816 -> 2159143701136
	2159143701136 [label=AccumulateGrad]
	2159143700752 -> 2159143700848
	2159142888896 [label="block1.3.conv1.1.weight
 (32)" fillcolor=lightblue]
	2159142888896 -> 2159143700752
	2159143700752 [label=AccumulateGrad]
	2159143700992 -> 2159143700848
	2159142888976 [label="block1.3.conv1.1.bias
 (32)" fillcolor=lightblue]
	2159142888976 -> 2159143700992
	2159143700992 [label=AccumulateGrad]
	2159143700608 -> 2159143700368
	2159142889376 [label="block1.3.conv2.0.weight
 (32, 32, 7, 7)" fillcolor=lightblue]
	2159142889376 -> 2159143700608
	2159143700608 [label=AccumulateGrad]
	2159143700560 -> 2159143700368
	2159142889456 [label="block1.3.conv2.0.bias
 (32)" fillcolor=lightblue]
	2159142889456 -> 2159143700560
	2159143700560 [label=AccumulateGrad]
	2159143700224 -> 2159143700320
	2159142889536 [label="block1.3.conv2.1.weight
 (32)" fillcolor=lightblue]
	2159142889536 -> 2159143700224
	2159143700224 [label=AccumulateGrad]
	2159143700464 -> 2159143700320
	2159142889616 [label="block1.3.conv2.1.bias
 (32)" fillcolor=lightblue]
	2159142889616 -> 2159143700464
	2159143700464 [label=AccumulateGrad]
	2159143700080 -> 2159143699936
	2159142890096 [label="block1.3.conv3.0.weight
 (128, 32, 3, 3)" fillcolor=lightblue]
	2159142890096 -> 2159143700080
	2159143700080 [label=AccumulateGrad]
	2159143700032 -> 2159143699936
	2159142890176 [label="block1.3.conv3.0.bias
 (128)" fillcolor=lightblue]
	2159142890176 -> 2159143700032
	2159143700032 [label=AccumulateGrad]
	2159143699888 -> 2159143699696
	2159142890256 [label="block1.3.conv3.1.weight
 (128)" fillcolor=lightblue]
	2159142890256 -> 2159143699888
	2159143699888 [label=AccumulateGrad]
	2159143699840 -> 2159143699696
	2159142890336 [label="block1.3.conv3.1.bias
 (128)" fillcolor=lightblue]
	2159142890336 -> 2159143699840
	2159143699840 [label=AccumulateGrad]
	2159143699552 -> 2159143699648
	2159143699408 -> 2159143699168
	2159142891536 [label="block2.0.conv1.0.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	2159142891536 -> 2159143699408
	2159143699408 [label=AccumulateGrad]
	2159143699360 -> 2159143699168
	2159142891616 [label="block2.0.conv1.0.bias
 (64)" fillcolor=lightblue]
	2159142891616 -> 2159143699360
	2159143699360 [label=AccumulateGrad]
	2159143699024 -> 2159143699120
	2159142891696 [label="block2.0.conv1.1.weight
 (64)" fillcolor=lightblue]
	2159142891696 -> 2159143699024
	2159143699024 [label=AccumulateGrad]
	2159143699264 -> 2159143699120
	2159142891776 [label="block2.0.conv1.1.bias
 (64)" fillcolor=lightblue]
	2159142891776 -> 2159143699264
	2159143699264 [label=AccumulateGrad]
	2159143698880 -> 2159143698640
	2159142892256 [label="block2.0.conv2.0.weight
 (64, 64, 7, 7)" fillcolor=lightblue]
	2159142892256 -> 2159143698880
	2159143698880 [label=AccumulateGrad]
	2159143698832 -> 2159143698640
	2159142892336 [label="block2.0.conv2.0.bias
 (64)" fillcolor=lightblue]
	2159142892336 -> 2159143698832
	2159143698832 [label=AccumulateGrad]
	2159143698496 -> 2159143698592
	2159142892416 [label="block2.0.conv2.1.weight
 (64)" fillcolor=lightblue]
	2159142892416 -> 2159143698496
	2159143698496 [label=AccumulateGrad]
	2159143698736 -> 2159143698592
	2159142892496 [label="block2.0.conv2.1.bias
 (64)" fillcolor=lightblue]
	2159142892496 -> 2159143698736
	2159143698736 [label=AccumulateGrad]
	2159155019632 -> 2159155019488
	2159142892976 [label="block2.0.conv3.0.weight
 (256, 64, 3, 3)" fillcolor=lightblue]
	2159142892976 -> 2159155019632
	2159155019632 [label=AccumulateGrad]
	2159155019584 -> 2159155019488
	2159142893056 [label="block2.0.conv3.0.bias
 (256)" fillcolor=lightblue]
	2159142893056 -> 2159155019584
	2159155019584 [label=AccumulateGrad]
	2159155019440 -> 2159155019248
	2159142893136 [label="block2.0.conv3.1.weight
 (256)" fillcolor=lightblue]
	2159142893136 -> 2159155019440
	2159155019440 [label=AccumulateGrad]
	2159155019392 -> 2159155019248
	2159142893216 [label="block2.0.conv3.1.bias
 (256)" fillcolor=lightblue]
	2159142893216 -> 2159155019392
	2159155019392 [label=AccumulateGrad]
	2159155019104 -> 2159155019200
	2159155019104 [label=NativeBatchNormBackward0]
	2159155019728 -> 2159155019104
	2159155019728 [label=ConvolutionBackward0]
	2159143699456 -> 2159155019728
	2159143699312 -> 2159155019728
	2159142890816 [label="block2.0.downsample.0.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2159142890816 -> 2159143699312
	2159143699312 [label=AccumulateGrad]
	2159143699072 -> 2159155019728
	2159142890896 [label="block2.0.downsample.0.bias
 (256)" fillcolor=lightblue]
	2159142890896 -> 2159143699072
	2159143699072 [label=AccumulateGrad]
	2159155019536 -> 2159155019104
	2159142890976 [label="block2.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2159142890976 -> 2159155019536
	2159155019536 [label=AccumulateGrad]
	2159143698544 -> 2159155019104
	2159142891056 [label="block2.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2159142891056 -> 2159143698544
	2159143698544 [label=AccumulateGrad]
	2159155019056 -> 2159155018768
	2159142893696 [label="block2.1.conv1.0.weight
 (64, 256, 3, 3)" fillcolor=lightblue]
	2159142893696 -> 2159155019056
	2159155019056 [label=AccumulateGrad]
	2159155019008 -> 2159155018768
	2159142893776 [label="block2.1.conv1.0.bias
 (64)" fillcolor=lightblue]
	2159142893776 -> 2159155019008
	2159155019008 [label=AccumulateGrad]
	2159155018624 -> 2159155018720
	2159142893856 [label="block2.1.conv1.1.weight
 (64)" fillcolor=lightblue]
	2159142893856 -> 2159155018624
	2159155018624 [label=AccumulateGrad]
	2159155018864 -> 2159155018720
	2159142893936 [label="block2.1.conv1.1.bias
 (64)" fillcolor=lightblue]
	2159142893936 -> 2159155018864
	2159155018864 [label=AccumulateGrad]
	2159155018480 -> 2159155018240
	2159142894416 [label="block2.1.conv2.0.weight
 (64, 64, 7, 7)" fillcolor=lightblue]
	2159142894416 -> 2159155018480
	2159155018480 [label=AccumulateGrad]
	2159155018432 -> 2159155018240
	2159142894496 [label="block2.1.conv2.0.bias
 (64)" fillcolor=lightblue]
	2159142894496 -> 2159155018432
	2159155018432 [label=AccumulateGrad]
	2159155018096 -> 2159155018192
	2159142894576 [label="block2.1.conv2.1.weight
 (64)" fillcolor=lightblue]
	2159142894576 -> 2159155018096
	2159155018096 [label=AccumulateGrad]
	2159155018336 -> 2159155018192
	2159142894656 [label="block2.1.conv2.1.bias
 (64)" fillcolor=lightblue]
	2159142894656 -> 2159155018336
	2159155018336 [label=AccumulateGrad]
	2159155017952 -> 2159155017808
	2159142895136 [label="block2.1.conv3.0.weight
 (256, 64, 3, 3)" fillcolor=lightblue]
	2159142895136 -> 2159155017952
	2159155017952 [label=AccumulateGrad]
	2159155017904 -> 2159155017808
	2159142895216 [label="block2.1.conv3.0.bias
 (256)" fillcolor=lightblue]
	2159142895216 -> 2159155017904
	2159155017904 [label=AccumulateGrad]
	2159155017760 -> 2159155017568
	2159142895296 [label="block2.1.conv3.1.weight
 (256)" fillcolor=lightblue]
	2159142895296 -> 2159155017760
	2159155017760 [label=AccumulateGrad]
	2159155017712 -> 2159155017568
	2159142895376 [label="block2.1.conv3.1.bias
 (256)" fillcolor=lightblue]
	2159142895376 -> 2159155017712
	2159155017712 [label=AccumulateGrad]
	2159155017424 -> 2159155017520
	2159155017376 -> 2159155017088
	2159143371056 [label="block2.2.conv1.0.weight
 (64, 256, 3, 3)" fillcolor=lightblue]
	2159143371056 -> 2159155017376
	2159155017376 [label=AccumulateGrad]
	2159155017328 -> 2159155017088
	2159143371136 [label="block2.2.conv1.0.bias
 (64)" fillcolor=lightblue]
	2159143371136 -> 2159155017328
	2159155017328 [label=AccumulateGrad]
	2159155016944 -> 2159155017040
	2159143371216 [label="block2.2.conv1.1.weight
 (64)" fillcolor=lightblue]
	2159143371216 -> 2159155016944
	2159155016944 [label=AccumulateGrad]
	2159155017184 -> 2159155017040
	2159143371296 [label="block2.2.conv1.1.bias
 (64)" fillcolor=lightblue]
	2159143371296 -> 2159155017184
	2159155017184 [label=AccumulateGrad]
	2159155016800 -> 2159155016560
	2159143371776 [label="block2.2.conv2.0.weight
 (64, 64, 7, 7)" fillcolor=lightblue]
	2159143371776 -> 2159155016800
	2159155016800 [label=AccumulateGrad]
	2159155016752 -> 2159155016560
	2159143371856 [label="block2.2.conv2.0.bias
 (64)" fillcolor=lightblue]
	2159143371856 -> 2159155016752
	2159155016752 [label=AccumulateGrad]
	2159155016416 -> 2159155016512
	2159143371936 [label="block2.2.conv2.1.weight
 (64)" fillcolor=lightblue]
	2159143371936 -> 2159155016416
	2159155016416 [label=AccumulateGrad]
	2159155016656 -> 2159155016512
	2159143372016 [label="block2.2.conv2.1.bias
 (64)" fillcolor=lightblue]
	2159143372016 -> 2159155016656
	2159155016656 [label=AccumulateGrad]
	2159155016272 -> 2159155016128
	2159143372496 [label="block2.2.conv3.0.weight
 (256, 64, 3, 3)" fillcolor=lightblue]
	2159143372496 -> 2159155016272
	2159155016272 [label=AccumulateGrad]
	2159155016224 -> 2159155016128
	2159143372576 [label="block2.2.conv3.0.bias
 (256)" fillcolor=lightblue]
	2159143372576 -> 2159155016224
	2159155016224 [label=AccumulateGrad]
	2159155016080 -> 2159155015888
	2159143372656 [label="block2.2.conv3.1.weight
 (256)" fillcolor=lightblue]
	2159143372656 -> 2159155016080
	2159155016080 [label=AccumulateGrad]
	2159155016032 -> 2159155015888
	2159143372736 [label="block2.2.conv3.1.bias
 (256)" fillcolor=lightblue]
	2159143372736 -> 2159155016032
	2159155016032 [label=AccumulateGrad]
	2159155015744 -> 2159155015840
	2159155015696 -> 2159155015408
	2159143373216 [label="block2.3.conv1.0.weight
 (64, 256, 3, 3)" fillcolor=lightblue]
	2159143373216 -> 2159155015696
	2159155015696 [label=AccumulateGrad]
	2159155015648 -> 2159155015408
	2159143373296 [label="block2.3.conv1.0.bias
 (64)" fillcolor=lightblue]
	2159143373296 -> 2159155015648
	2159155015648 [label=AccumulateGrad]
	2159155015264 -> 2159155015360
	2159143373376 [label="block2.3.conv1.1.weight
 (64)" fillcolor=lightblue]
	2159143373376 -> 2159155015264
	2159155015264 [label=AccumulateGrad]
	2159155015504 -> 2159155015360
	2159143373456 [label="block2.3.conv1.1.bias
 (64)" fillcolor=lightblue]
	2159143373456 -> 2159155015504
	2159155015504 [label=AccumulateGrad]
	2159155015120 -> 2159155014880
	2159143373936 [label="block2.3.conv2.0.weight
 (64, 64, 7, 7)" fillcolor=lightblue]
	2159143373936 -> 2159155015120
	2159155015120 [label=AccumulateGrad]
	2159155015072 -> 2159155014880
	2159143374016 [label="block2.3.conv2.0.bias
 (64)" fillcolor=lightblue]
	2159143374016 -> 2159155015072
	2159155015072 [label=AccumulateGrad]
	2159155014736 -> 2159155014832
	2159143374096 [label="block2.3.conv2.1.weight
 (64)" fillcolor=lightblue]
	2159143374096 -> 2159155014736
	2159155014736 [label=AccumulateGrad]
	2159155014976 -> 2159155014832
	2159143374176 [label="block2.3.conv2.1.bias
 (64)" fillcolor=lightblue]
	2159143374176 -> 2159155014976
	2159155014976 [label=AccumulateGrad]
	2159155014592 -> 2159155014448
	2159143374656 [label="block2.3.conv3.0.weight
 (256, 64, 3, 3)" fillcolor=lightblue]
	2159143374656 -> 2159155014592
	2159155014592 [label=AccumulateGrad]
	2159155014544 -> 2159155014448
	2159143374736 [label="block2.3.conv3.0.bias
 (256)" fillcolor=lightblue]
	2159143374736 -> 2159155014544
	2159155014544 [label=AccumulateGrad]
	2159155014400 -> 2159155014208
	2159143374816 [label="block2.3.conv3.1.weight
 (256)" fillcolor=lightblue]
	2159143374816 -> 2159155014400
	2159155014400 [label=AccumulateGrad]
	2159155014352 -> 2159155014208
	2159143374896 [label="block2.3.conv3.1.bias
 (256)" fillcolor=lightblue]
	2159143374896 -> 2159155014352
	2159155014352 [label=AccumulateGrad]
	2159155014064 -> 2159155014160
	2159155014016 -> 2159155013776
	2159143375376 [label="block2.4.conv1.0.weight
 (64, 256, 3, 3)" fillcolor=lightblue]
	2159143375376 -> 2159155014016
	2159155014016 [label=AccumulateGrad]
	2159155013968 -> 2159155013776
	2159143375456 [label="block2.4.conv1.0.bias
 (64)" fillcolor=lightblue]
	2159143375456 -> 2159155013968
	2159155013968 [label=AccumulateGrad]
	2159155013632 -> 2159155013728
	2159143375536 [label="block2.4.conv1.1.weight
 (64)" fillcolor=lightblue]
	2159143375536 -> 2159155013632
	2159155013632 [label=AccumulateGrad]
	2159155013872 -> 2159155013728
	2159143375616 [label="block2.4.conv1.1.bias
 (64)" fillcolor=lightblue]
	2159143375616 -> 2159155013872
	2159155013872 [label=AccumulateGrad]
	2159155013488 -> 2159155013248
	2159143376096 [label="block2.4.conv2.0.weight
 (64, 64, 7, 7)" fillcolor=lightblue]
	2159143376096 -> 2159155013488
	2159155013488 [label=AccumulateGrad]
	2159155013440 -> 2159155013248
	2159143376176 [label="block2.4.conv2.0.bias
 (64)" fillcolor=lightblue]
	2159143376176 -> 2159155013440
	2159155013440 [label=AccumulateGrad]
	2159155013104 -> 2159155013200
	2159143376256 [label="block2.4.conv2.1.weight
 (64)" fillcolor=lightblue]
	2159143376256 -> 2159155013104
	2159155013104 [label=AccumulateGrad]
	2159155013344 -> 2159155013200
	2159143376336 [label="block2.4.conv2.1.bias
 (64)" fillcolor=lightblue]
	2159143376336 -> 2159155013344
	2159155013344 [label=AccumulateGrad]
	2159155012960 -> 2159155012816
	2159143376816 [label="block2.4.conv3.0.weight
 (256, 64, 3, 3)" fillcolor=lightblue]
	2159143376816 -> 2159155012960
	2159155012960 [label=AccumulateGrad]
	2159155012912 -> 2159155012816
	2159143376896 [label="block2.4.conv3.0.bias
 (256)" fillcolor=lightblue]
	2159143376896 -> 2159155012912
	2159155012912 [label=AccumulateGrad]
	2159155012768 -> 2159155012576
	2159143376976 [label="block2.4.conv3.1.weight
 (256)" fillcolor=lightblue]
	2159143376976 -> 2159155012768
	2159155012768 [label=AccumulateGrad]
	2159155012720 -> 2159155012576
	2159143377056 [label="block2.4.conv3.1.bias
 (256)" fillcolor=lightblue]
	2159143377056 -> 2159155012720
	2159155012720 [label=AccumulateGrad]
	2159155012432 -> 2159155012528
	2159155012384 -> 2159155012096
	2159143377536 [label="block2.5.conv1.0.weight
 (64, 256, 3, 3)" fillcolor=lightblue]
	2159143377536 -> 2159155012384
	2159155012384 [label=AccumulateGrad]
	2159155012336 -> 2159155012096
	2159143377616 [label="block2.5.conv1.0.bias
 (64)" fillcolor=lightblue]
	2159143377616 -> 2159155012336
	2159155012336 [label=AccumulateGrad]
	2159155011952 -> 2159155012048
	2159143377696 [label="block2.5.conv1.1.weight
 (64)" fillcolor=lightblue]
	2159143377696 -> 2159155011952
	2159155011952 [label=AccumulateGrad]
	2159155012192 -> 2159155012048
	2159143377776 [label="block2.5.conv1.1.bias
 (64)" fillcolor=lightblue]
	2159143377776 -> 2159155012192
	2159155012192 [label=AccumulateGrad]
	2159155011808 -> 2159155011568
	2159143378256 [label="block2.5.conv2.0.weight
 (64, 64, 7, 7)" fillcolor=lightblue]
	2159143378256 -> 2159155011808
	2159155011808 [label=AccumulateGrad]
	2159155011760 -> 2159155011568
	2159143378336 [label="block2.5.conv2.0.bias
 (64)" fillcolor=lightblue]
	2159143378336 -> 2159155011760
	2159155011760 [label=AccumulateGrad]
	2159155011424 -> 2159155011520
	2159143378416 [label="block2.5.conv2.1.weight
 (64)" fillcolor=lightblue]
	2159143378416 -> 2159155011424
	2159155011424 [label=AccumulateGrad]
	2159155011664 -> 2159155011520
	2159143378496 [label="block2.5.conv2.1.bias
 (64)" fillcolor=lightblue]
	2159143378496 -> 2159155011664
	2159155011664 [label=AccumulateGrad]
	2159155011280 -> 2159155011136
	2159143378976 [label="block2.5.conv3.0.weight
 (256, 64, 3, 3)" fillcolor=lightblue]
	2159143378976 -> 2159155011280
	2159155011280 [label=AccumulateGrad]
	2159155011232 -> 2159155011136
	2159143379056 [label="block2.5.conv3.0.bias
 (256)" fillcolor=lightblue]
	2159143379056 -> 2159155011232
	2159155011232 [label=AccumulateGrad]
	2159155011088 -> 2159155010896
	2159143379136 [label="block2.5.conv3.1.weight
 (256)" fillcolor=lightblue]
	2159143379136 -> 2159155011088
	2159155011088 [label=AccumulateGrad]
	2159155011040 -> 2159155010896
	2159143379216 [label="block2.5.conv3.1.bias
 (256)" fillcolor=lightblue]
	2159143379216 -> 2159155011040
	2159155011040 [label=AccumulateGrad]
	2159155010752 -> 2159155010848
	2159155010608 -> 2159155010368
	2159143380256 [label="block3.0.conv1.0.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	2159143380256 -> 2159155010608
	2159155010608 [label=AccumulateGrad]
	2159155010560 -> 2159155010368
	2159143380336 [label="block3.0.conv1.0.bias
 (128)" fillcolor=lightblue]
	2159143380336 -> 2159155010560
	2159155010560 [label=AccumulateGrad]
	2159155010224 -> 2159155010320
	2159143380416 [label="block3.0.conv1.1.weight
 (128)" fillcolor=lightblue]
	2159143380416 -> 2159155010224
	2159155010224 [label=AccumulateGrad]
	2159155010464 -> 2159155010320
	2159143380496 [label="block3.0.conv1.1.bias
 (128)" fillcolor=lightblue]
	2159143380496 -> 2159155010464
	2159155010464 [label=AccumulateGrad]
	2159155010080 -> 2159155009840
	2159143380896 [label="block3.0.conv2.0.weight
 (128, 128, 7, 7)" fillcolor=lightblue]
	2159143380896 -> 2159155010080
	2159155010080 [label=AccumulateGrad]
	2159155010032 -> 2159155009840
	2159143380976 [label="block3.0.conv2.0.bias
 (128)" fillcolor=lightblue]
	2159143380976 -> 2159155010032
	2159155010032 [label=AccumulateGrad]
	2159155009696 -> 2159155009792
	2159143381056 [label="block3.0.conv2.1.weight
 (128)" fillcolor=lightblue]
	2159143381056 -> 2159155009696
	2159155009696 [label=AccumulateGrad]
	2159155009936 -> 2159155009792
	2159143381136 [label="block3.0.conv2.1.bias
 (128)" fillcolor=lightblue]
	2159143381136 -> 2159155009936
	2159155009936 [label=AccumulateGrad]
	2159155009552 -> 2159155009408
	2159143381536 [label="block3.0.conv3.0.weight
 (512, 128, 3, 3)" fillcolor=lightblue]
	2159143381536 -> 2159155009552
	2159155009552 [label=AccumulateGrad]
	2159155009504 -> 2159155009408
	2159143381616 [label="block3.0.conv3.0.bias
 (512)" fillcolor=lightblue]
	2159143381616 -> 2159155009504
	2159155009504 [label=AccumulateGrad]
	2159155009360 -> 2159155009168
	2159143381696 [label="block3.0.conv3.1.weight
 (512)" fillcolor=lightblue]
	2159143381696 -> 2159155009360
	2159155009360 [label=AccumulateGrad]
	2159155009312 -> 2159155009168
	2159143381776 [label="block3.0.conv3.1.bias
 (512)" fillcolor=lightblue]
	2159143381776 -> 2159155009312
	2159155009312 [label=AccumulateGrad]
	2159155009024 -> 2159155009120
	2159155009024 [label=NativeBatchNormBackward0]
	2159155009744 -> 2159155009024
	2159155009744 [label=ConvolutionBackward0]
	2159155010656 -> 2159155009744
	2159155010512 -> 2159155009744
	2159143379616 [label="block3.0.downsample.0.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2159143379616 -> 2159155010512
	2159155010512 [label=AccumulateGrad]
	2159155010272 -> 2159155009744
	2159143379696 [label="block3.0.downsample.0.bias
 (512)" fillcolor=lightblue]
	2159143379696 -> 2159155010272
	2159155010272 [label=AccumulateGrad]
	2159155009648 -> 2159155009024
	2159143379776 [label="block3.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2159143379776 -> 2159155009648
	2159155009648 [label=AccumulateGrad]
	2159155009456 -> 2159155009024
	2159143379856 [label="block3.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2159143379856 -> 2159155009456
	2159155009456 [label=AccumulateGrad]
	2159155008976 -> 2159155008688
	2159143382256 [label="block3.1.conv1.0.weight
 (128, 512, 3, 3)" fillcolor=lightblue]
	2159143382256 -> 2159155008976
	2159155008976 [label=AccumulateGrad]
	2159155008928 -> 2159155008688
	2159143382336 [label="block3.1.conv1.0.bias
 (128)" fillcolor=lightblue]
	2159143382336 -> 2159155008928
	2159155008928 [label=AccumulateGrad]
	2159155008544 -> 2159155008640
	2159143382416 [label="block3.1.conv1.1.weight
 (128)" fillcolor=lightblue]
	2159143382416 -> 2159155008544
	2159155008544 [label=AccumulateGrad]
	2159155008784 -> 2159155008640
	2159143382496 [label="block3.1.conv1.1.bias
 (128)" fillcolor=lightblue]
	2159143382496 -> 2159155008784
	2159155008784 [label=AccumulateGrad]
	2159155008400 -> 2159155008256
	2159143382976 [label="block3.1.conv2.0.weight
 (128, 128, 7, 7)" fillcolor=lightblue]
	2159143382976 -> 2159155008400
	2159155008400 [label=AccumulateGrad]
	2159155008352 -> 2159155008256
	2159143383056 [label="block3.1.conv2.0.bias
 (128)" fillcolor=lightblue]
	2159143383056 -> 2159155008352
	2159155008352 [label=AccumulateGrad]
	2159155008208 -> 2159155008160
	2159143383136 [label="block3.1.conv2.1.weight
 (128)" fillcolor=lightblue]
	2159143383136 -> 2159155008208
	2159155008208 [label=AccumulateGrad]
	2159155008064 -> 2159155008160
	2159143383216 [label="block3.1.conv2.1.bias
 (128)" fillcolor=lightblue]
	2159143383216 -> 2159155008064
	2159155008064 [label=AccumulateGrad]
	2159155007920 -> 2159155007776
	2159143383696 [label="block3.1.conv3.0.weight
 (512, 128, 3, 3)" fillcolor=lightblue]
	2159143383696 -> 2159155007920
	2159155007920 [label=AccumulateGrad]
	2159155007872 -> 2159155007776
	2159143383776 [label="block3.1.conv3.0.bias
 (512)" fillcolor=lightblue]
	2159143383776 -> 2159155007872
	2159155007872 [label=AccumulateGrad]
	2159155007728 -> 2159155007632
	2159143383856 [label="block3.1.conv3.1.weight
 (512)" fillcolor=lightblue]
	2159143383856 -> 2159155007728
	2159155007728 [label=AccumulateGrad]
	2159155007680 -> 2159155007632
	2159143383936 [label="block3.1.conv3.1.bias
 (512)" fillcolor=lightblue]
	2159143383936 -> 2159155007680
	2159155007680 [label=AccumulateGrad]
	2159155007584 -> 2159155007536
	2159155007392 -> 2159155007200
	2159143384416 [label="block3.2.conv1.0.weight
 (128, 512, 3, 3)" fillcolor=lightblue]
	2159143384416 -> 2159155007392
	2159155007392 [label=AccumulateGrad]
	2159155007344 -> 2159155007200
	2159143384496 [label="block3.2.conv1.0.bias
 (128)" fillcolor=lightblue]
	2159143384496 -> 2159155007344
	2159155007344 [label=AccumulateGrad]
	2159155007152 -> 2159155007104
	2159143384576 [label="block3.2.conv1.1.weight
 (128)" fillcolor=lightblue]
	2159143384576 -> 2159155007152
	2159155007152 [label=AccumulateGrad]
	2159155007008 -> 2159155007104
	2159143384656 [label="block3.2.conv1.1.bias
 (128)" fillcolor=lightblue]
	2159143384656 -> 2159155007008
	2159155007008 [label=AccumulateGrad]
	2159155006864 -> 2159155006720
	2159143385136 [label="block3.2.conv2.0.weight
 (128, 128, 7, 7)" fillcolor=lightblue]
	2159143385136 -> 2159155006864
	2159155006864 [label=AccumulateGrad]
	2159155006816 -> 2159155006720
	2159143385216 [label="block3.2.conv2.0.bias
 (128)" fillcolor=lightblue]
	2159143385216 -> 2159155006816
	2159155006816 [label=AccumulateGrad]
	2159155006672 -> 2159155006624
	2159143385296 [label="block3.2.conv2.1.weight
 (128)" fillcolor=lightblue]
	2159143385296 -> 2159155006672
	2159155006672 [label=AccumulateGrad]
	2159155006528 -> 2159155006624
	2159143385376 [label="block3.2.conv2.1.bias
 (128)" fillcolor=lightblue]
	2159143385376 -> 2159155006528
	2159155006528 [label=AccumulateGrad]
	2159155006384 -> 2159155006240
	2159143385776 [label="block3.2.conv3.0.weight
 (512, 128, 3, 3)" fillcolor=lightblue]
	2159143385776 -> 2159155006384
	2159155006384 [label=AccumulateGrad]
	2159155006336 -> 2159155006240
	2159143385856 [label="block3.2.conv3.0.bias
 (512)" fillcolor=lightblue]
	2159143385856 -> 2159155006336
	2159155006336 [label=AccumulateGrad]
	2159155006192 -> 2159155006096
	2159143385936 [label="block3.2.conv3.1.weight
 (512)" fillcolor=lightblue]
	2159143385936 -> 2159155006192
	2159155006192 [label=AccumulateGrad]
	2159155006144 -> 2159155006096
	2159143386016 [label="block3.2.conv3.1.bias
 (512)" fillcolor=lightblue]
	2159143386016 -> 2159155006144
	2159155006144 [label=AccumulateGrad]
	2159155006048 -> 2159155006000
	2159155005616 -> 2159155005280
	2159155005616 [label=TBackward0]
	2159155005856 -> 2159155005616
	2159093466032 [label="fc.weight
 (10, 512)" fillcolor=lightblue]
	2159093466032 -> 2159155005856
	2159155005856 [label=AccumulateGrad]
	2159155005280 -> 2159155052928
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet34 Model for Fashion MNIST\n",
    "This model is the realization of the first object of this project, which was to create a deep neural network model which, when trained on the Fashion MNIST training dataset, could achieve an accuracy of 90% when classifying on the corresponding testing data.\n",
    "\n",
    "This implementation is one called a residual network (ResNet). Most other networks 'linearily' pass down the information of an image through various levels of convulational filters and pooling layers. The uniquesnss of a ResNet is that it sums the output of convulational layers within 'blocks' of them as the features travel through. That way there is no issue of disappearing gradient, increasing the accuracy and performance of the model. \n",
    "\n",
    "Along with the arcitecture of the model, this notebook contains training and testing of the network, as applied to the Fashion MNIST data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Much of the resources required for the creation and evaluation of the model comes from Pytorch. Even the use of the FashionMNIST dataset is imported from Pytorch's version of it. \n",
    "\n",
    "For the visualisation of the results, the pandas and matplotlib's pyplot were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "from typing import Tuple\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "# from sklearn.metrics import confusion_matrix, top_k_accuracy_score\n",
    "import torchvision                                                       \n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import ToTensor\n",
    "from numpy import random as rd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gc\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader Block\n",
    "Here, the Fashion MNIST dataset is loaded from torch's dataset library and split into its training and testing sets. The only transformation applied a ToTensor, for technical reasons. Any real image transformation would obscure information from an already low resolution, grayscale picture, with little performance benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform method\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "# train data\n",
    "trainData = datasets.FashionMNIST(root=\"./\",\n",
    "                                  train=True,\n",
    "                                  transform=transform,\n",
    "                                  download=True\n",
    "                                  )\n",
    "trainLoad = DataLoader(trainData, \n",
    "                       batch_size=30, \n",
    "                       shuffle=True, \n",
    "                       drop_last=False\n",
    "                       )\n",
    "# test data\n",
    "testData = datasets.FashionMNIST(root=\"./\",\n",
    "                                  train=False,\n",
    "                                  transform=transform,\n",
    "                                  download=True\n",
    "                                  )\n",
    "testLoad = DataLoader(testData, \n",
    "                     batch_size=30, \n",
    "                     shuffle=True, \n",
    "                     drop_last=False\n",
    "                     )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Block\n",
    "The residual block holds the basic essence of the layers that compose the ResNet. The order by which each layer is initialized is the same order the layers are put in when forward is called and the model is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \n",
    "    expansion = 4   # factor by which to expand the number of features per block\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # First block's convolutional layer with a batch normalization and RELU activation\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Second block's convolutional layer with batch normalization and RELU activation\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=7, stride=stride, padding=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Third block's convolutional layer with batch normalization\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels*self.expansion)\n",
    "        )\n",
    "\n",
    "        #Finishing layers, with a downsample and activation\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_channels=out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        if self.downsample != None:\n",
    "            residual = self.downsample(residual)\n",
    "            \n",
    "        x += residual\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renset Block\n",
    "This is the general model and the system by which the residual blocks are inserted. First is a input convolutional layer, followed by the layers of residual blocks, then a average pool to finish off with a fully connected linear layer to output the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for ResNet model that extend from nn.Module\n",
    "class Resnet(nn.Module):\n",
    "    \n",
    "    # initialize the resnet model with inputted block type, list of blockNum \n",
    "    def __init__(self, block, blockList, input_num, output_num):\n",
    "        super(Resnet, self).__init__()\n",
    "        \n",
    "        self.in_channels = 16  # Standard factor of feature channels to expand each block from\n",
    "        \n",
    "        # First convulotion layer with batch normalization, ReLU activation, and max pooling as this is the first layer from input\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(input_num, 16, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        # Calling of the make layer functions to build middle of the model\n",
    "        self.block0 = self._make_layer(block,  out_channels=16, blocksNum=blockList[0], stride=1)\n",
    "        self.block1 = self._make_layer(block,  out_channels=32, blocksNum=blockList[1], stride=2)\n",
    "        self.block2 = self._make_layer(block,  out_channels=64, blocksNum=blockList[2], stride=2)\n",
    "        self.block3 = self._make_layer(block,  out_channels=128, blocksNum=blockList[3], stride=2)\n",
    "        \n",
    "        # apply 2D adaptive average pooling from 1 input to 1 plane\n",
    "        # self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.avgpool = nn.AvgPool2d(1, 1)\n",
    "        \n",
    "        # flatten the data into 1 dimension\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # apply dropout to output with 60% percent chance\n",
    "        self.drop = nn.Dropout(0.6)\n",
    "        \n",
    "        # connect 2048 input nodes into 10 output nodes\n",
    "        self.fc = nn.Linear(128*4, output_num)\n",
    "\n",
    "    # helper function that adds layer by layer along with the res block\n",
    "    def _make_layer(self, block: ResidualBlock, out_channels, blocksNum, stride):\n",
    "        downn_sample = None\n",
    "        \n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downn_sample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=3, stride=stride, padding=1),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        # layer that change the number of out channel, in will be inputed out\n",
    "        layers.append(block(self.in_channels, out_channels, stride=stride, downsample=downn_sample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        \n",
    "        # connected large output to smaller out \n",
    "        for _ in range(1, blocksNum):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    # forward function \n",
    "    def forward(self, x: ToTensor):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.block0(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boiler Plate (Traing/Testing)\n",
    "The method below is our primary means of training, testing, and evaluating the model. The arguments are the hyperparameters set for the use of the model. The model will run over the specified epochs with the batch size with optimization. Accuracy can be tested over the epochs from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boilerplate(ep, lr, wdr, mom):    \n",
    "    # number of epoch\n",
    "    epochNum=ep\n",
    "    # learning rate\n",
    "    learningRate = lr\n",
    "    # weight decay\n",
    "    weightDecayRate = wdr\n",
    "    # momentum\n",
    "    momentumAmount = mom\n",
    "    # setting up the device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # setting up the model\n",
    "    # block using the ResidualBlock\n",
    "    # blockNums using the inputted list\n",
    "    # input_num of 1 for gray scaled, 3 for color\n",
    "    # output_num of 10 for 10 classes\n",
    "    model = Resnet(ResidualBlock, [3, 4, 6, 3], 1, 10).to(device)\n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=weightDecayRate)\n",
    "    total_step = len(trainLoad)\n",
    "    # print(model)\n",
    "    for epoch in range(epochNum):\n",
    "        for i, (images, labels) in enumerate(trainLoad):\n",
    "            # move tensor to device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # forward the output and calculate loss\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # backward the output and perform optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # deallocation\n",
    "            del images, labels, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochNum, loss.item()))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in testLoad:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "        \n",
    "       # print('Accuracy of the network on the {} validation images: {} %'.format(10000, 100 * correct / total))\n",
    "    return correct, total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Method\n",
    "The below code runs everything in the notebook, from creating the netork, to training/testing it, as well as outputting nessesary metrics. Results are written to the text document 'results.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m rnResults \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mresults.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 25\u001b[0m correct, total \u001b[39m=\u001b[39m boilerplate(\u001b[39m30\u001b[39;49m, \u001b[39m.005\u001b[39;49m, \u001b[39m.001\u001b[39;49m, \u001b[39m.5\u001b[39;49m)\n\u001b[0;32m     26\u001b[0m runTime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m     27\u001b[0m hours \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mfloor(runTime \u001b[39m/\u001b[39m \u001b[39m3600\u001b[39m))\n",
      "Cell \u001b[1;32mIn[29], line 36\u001b[0m, in \u001b[0;36mboilerplate\u001b[1;34m(ep, lr, wdr, mom)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39m# backward the output and perform optimization\u001b[39;00m\n\u001b[0;32m     35\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 36\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     37\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     39\u001b[0m \u001b[39m# deallocation\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "batch = 2048\n",
    "while batch <= 2049:\n",
    "         trainData = datasets.FashionMNIST(root=\"./\",\n",
    "                                  train=True,\n",
    "                                  transform=transform,\n",
    "                                  download=False\n",
    "                                  )\n",
    "         trainLoad = DataLoader(trainData, \n",
    "                                batch_size=batch, \n",
    "                            shuffle=True, \n",
    "                               drop_last=False\n",
    "                            )\n",
    "         testData = datasets.FashionMNIST(root=\"./\",\n",
    "                                  train=False,\n",
    "                                  transform=transform,\n",
    "                                  download=False\n",
    "                                  )\n",
    "         testLoad = DataLoader(testData, \n",
    "                     batch_size=batch, \n",
    "                     shuffle=True,\n",
    "                     drop_last=False\n",
    "                     )\n",
    "         rnResults = open('results.txt', 'a')\n",
    "         start_time = time.time()\n",
    "         correct, total = boilerplate(30, .005, .001, .5)\n",
    "         runTime = time.time() - start_time\n",
    "         hours = int(np.floor(runTime / 3600))\n",
    "         mins = int(np.floor((runTime - (hours * 3600)) /60))\n",
    "         secs = ((runTime - (hours * 3600)) - (mins * 60))\n",
    "         rnResults.write(\"Epoch: \" + str(30) +\"\\nBatch: \" + str(batch) + \"\\n\")\n",
    "         rnResults.write('Accuracy of the network on the {} validation images: {} %'.format(10000, 100 * correct / total))\n",
    "         rnResults.write(\"\\nRuntime: \" + str(hours) + \":\" + str(mins) + \":\" + str(secs) +\"\\n\")\n",
    "         rnResults.write(\"\\n\")\n",
    "         rnResults.write(\"------------------END BATCH \" + str(batch) + \"------------------\\n\\n------------------BEGIN EPOCH \" + str(batch * 2) +\"------------------\\n\")\n",
    "         rnResults.close()\n",
    "         batch = batch * 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c91dd610",
   "metadata": {},
   "source": [
    "## Denisha's Experiments and Research\n",
    "This notebook is experimentation and implemenation of a simple neural network applied to Fashion MNIST. The purpose was to learn ML basics and start working with Fashion MNIST in earnist. This notebook is an continuation and refinement of the model in the Denisha's first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38df8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the FMNIST dataset\n",
    "train_dataset = datasets.FashionMNIST(root='C:\\\\Users\\\\denis\\\\CACI', train=True, download=True,\n",
    "                                      transform=transforms.ToTensor())\n",
    "\n",
    "# Convert the training set to numpy arrays\n",
    "train_data = train_dataset.data.numpy()\n",
    "train_labels = train_dataset.targets.numpy()\n",
    "\n",
    "# Reshape the data to 2D (number of samples x number of features)\n",
    "train_data_reshaped = train_data.reshape(train_data.shape[0], -1)\n",
    "\n",
    "# Perform KMeans clustering\n",
    "n_clusters = 30000\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(train_data_reshaped)\n",
    "\n",
    "# Find the nearest neighbor for each cluster center\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "subset_indices = []\n",
    "for center_idx in range(n_clusters):\n",
    "    center = cluster_centers[center_idx]\n",
    "    center_distances = np.linalg.norm(train_data_reshaped - center, axis=1)\n",
    "    nearest_indices = np.argpartition(center_distances, 3000)[:3000]\n",
    "    subset_indices.extend(nearest_indices)\n",
    "\n",
    "# Define the data loader for the subset dataset\n",
    "batch_size = 64\n",
    "subset_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                            sampler=torch.utils.data.SubsetRandomSampler(subset_indices))\n",
    "\n",
    "# Define the baseline CNN classifier architecture\n",
    "class CNN(torch.nn.Module):\n",
    "    # ... model definition ...\n",
    "\n",
    "# Instantiate the CNN model, criterion, optimizer, and device\n",
    "    num_classes = len(train_dataset.classes)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    classifier = CNN().to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(classifier.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "# Train the baseline classifier on the subset dataset\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(subset_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch [%d/%d], Loss: %.4f' % (epoch + 1, num_epochs, running_loss / len(subset_loader)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesis\n",
    "The main method of synthesis implemented here is \"Gradient Matching\" derived from [this](https://openreview.net/pdf?id=mSAKhLYLSsl) paper and code from [this](https://github.com/dm-medvedev/EfficientDistillation) repository. In a nutshell, the synthesized imageset is iteratively developed by taking sample minibatches of the training data, in this case Fashin MNIST, and computing the loss by applying a deep neural network. The associated batch of synthetic data with the samples is then updated by using gradient descent to match the change in loss with the training sample's. The parameters used on the deep neural network is updated to minimize loss after each iteration in which the synthetic data is updated. The end result is a fully synthesized data set. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "The primary resource used is PyTorch's 'torchvision' and module tools. Additionally, seperate utility methods are used for dataset and external network managament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "from utils import get_loops, get_dataset, get_network, get_eval_pool, evaluate_synset, get_daparam, match_loss, get_time, TensorDataset, epoch, DiffAugment, ParamDiffAug"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Variables\n",
    "Below are the variables nessessary to change the method used, certain hyperparameters, and other functionalities of the synthesis method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method DC/DSA\n",
    "method='DC'\n",
    "# image(s) per class\n",
    "image_per_class=60\n",
    "# the number of evaluating randomly initialized models\n",
    "num_eval=20\n",
    "# epochs to train a model with synthetic data\n",
    "epoch_eval_train=5\n",
    "# training iterations\n",
    "training_iteration=1\n",
    "# learning rate for updating synthetic images\n",
    "lr_img=0.1\n",
    "# learning rate for updating network parameters\n",
    "lr_net=0.01\n",
    "# batch size for test data\n",
    "batch_test=64\n",
    "# batch size for training networks\n",
    "batch_train=64\n",
    "# noise/real: initialize synthetic images from random noise or randomly sampled real images\n",
    "synthetic_init='real'\n",
    "# differentiable Siamese augmentation strategy\n",
    "dsa_strategy=None\n",
    "# dataset path\n",
    "data_path='data'\n",
    "# path to save results\n",
    "save_path='result'\n",
    "# distance metric\n",
    "dis_metric='ours'\n",
    "\n",
    "outer_loop, inner_loop = get_loops(image_per_class)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dsa_param = ParamDiffAug()\n",
    "dsa = True if method == 'DSA' else False\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "eval_it_pool = np.arange(0, training_iteration+1, 500).tolist() \n",
    "print('eval_it_pool: ', eval_it_pool)\n",
    "channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = get_dataset(data_path)\n",
    "model_eval_pool = get_eval_pool('ResNet34')\n",
    "\n",
    "\n",
    "accs_all_exps = dict() # record performances of all experiments\n",
    "for key in model_eval_pool:\n",
    "    accs_all_exps[key] = []\n",
    "\n",
    "data_save = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST\n",
    "Turns Fashion MNIST into usable objects for the purpose of training/testing as well as splitting the dataset into minibatches for iterative training/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_all = []\n",
    "labels_all = []\n",
    "indices_class = [[] for c in range(num_classes)]\n",
    "\n",
    "images_all = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))]\n",
    "labels_all = [dst_train[i][1] for i in range(len(dst_train))]\n",
    "for i, lab in enumerate(labels_all):\n",
    "    indices_class[lab].append(i)\n",
    "images_all = torch.cat(images_all, dim=0).to(device)\n",
    "labels_all = torch.tensor(labels_all, dtype=torch.long, device=device)\n",
    "\n",
    "for c in range(num_classes):\n",
    "    print('class c = %d: %d real images'%(c, len(indices_class[c])))\n",
    "\n",
    "def get_images(c, n): # get random n images from class c\n",
    "    idx_shuffle = np.random.permutation(indices_class[c])[:n]\n",
    "    return images_all[idx_shuffle]\n",
    "\n",
    "for ch in range(channel):\n",
    "    print('real images channel %d, mean = %.4f, std = %.4f'%(ch, torch.mean(images_all[:, ch]), torch.std(images_all[:, ch])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data\n",
    "The initialized synthetic data matches the dimensions of the Fashion MNIST images. The number of memebers is initialized here as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' initialize the synthetic data '''\n",
    "image_syn = torch.randn(size=(num_classes*image_per_class, channel, im_size[0], im_size[1]), dtype=torch.float, requires_grad=True, device=device)\n",
    "label_syn = torch.tensor([np.ones(image_per_class)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]\n",
    "\n",
    "if synthetic_init == 'real':\n",
    "    print('initialize synthetic data from random real images')\n",
    "    for c in range(num_classes):\n",
    "        image_syn.data[c*image_per_class:(c+1)*image_per_class] = get_images(c, image_per_class).detach().data\n",
    "else:\n",
    "    print('initialize synthetic data from random noise')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesis Process\n",
    "Using the optimizer algorithms and loss criterion metrics as a basis, the general algorithm of gradient matching is implemented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' training '''\n",
    "optimizer_img = torch.optim.SGD([image_syn, ], lr=lr_img, momentum=0.5) # optimizer_img for synthetic data\n",
    "optimizer_img.zero_grad()\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "print('%s training begins'%get_time())\n",
    "\n",
    "for it in range(training_iteration+1):\n",
    "\n",
    "    ''' Evaluate synthetic data '''\n",
    "    if it in eval_it_pool:\n",
    "        for model_eval in model_eval_pool:\n",
    "            print('-------------------------\\nEvaluation\\nmodel_train = %s, model_eval = %s, iteration = %d'%('ResNet34', model_eval, it))\n",
    "            if dsa:\n",
    "                epoch_eval_train = 1000\n",
    "                dc_aug_param = None\n",
    "                print('DSA augmentation strategy: \\n', dsa_strategy)\n",
    "                print('DSA augmentation parameters: \\n', dsa_param.__dict__)\n",
    "            else:\n",
    "                dc_aug_param = get_daparam() # This augmentation parameter set is only for DC method. It will be muted when args.dsa is True.\n",
    "                print('DC augmentation parameters: \\n', dc_aug_param)\n",
    "\n",
    "            if dsa or dc_aug_param['strategy'] != 'none':\n",
    "                epoch_eval_train = 60  # Training with data augmentation needs more epochs.\n",
    "            else:\n",
    "                epoch_eval_train = 30\n",
    "\n",
    "            accs = []\n",
    "            for it_eval in range(num_eval):\n",
    "                net_eval = get_network(channel, num_classes, im_size).to(device) # get a random model\n",
    "                image_syn_eval, label_syn_eval = copy.deepcopy(image_syn.detach()), copy.deepcopy(label_syn.detach()) # avoid any unaware modification\n",
    "                _, acc_train, acc_test = evaluate_synset(it_eval, net_eval, image_syn_eval, label_syn_eval, testloader, device, lr_net, epoch_eval_train, batch_train, dsa, dsa_strategy, dsa_param, dc_aug_param)\n",
    "                accs.append(acc_test)\n",
    "            print('Evaluate %d random %s, mean = %.4f std = %.4f\\n-------------------------'%(len(accs), model_eval, np.mean(accs), np.std(accs)))\n",
    "\n",
    "            if it == training_iteration: # record the final results\n",
    "                accs_all_exps[model_eval] += accs\n",
    "\n",
    "        ''' visualize and save '''\n",
    "        save_name = os.path.join(save_path, 'vis_%s_%s_%s_%dipc_iter%d.png'%(method, 'FashionMNIST', 'ResNet34', image_per_class, it))\n",
    "        image_syn_vis = copy.deepcopy(image_syn.detach().cpu())\n",
    "        for ch in range(channel):\n",
    "            image_syn_vis[:, ch] = image_syn_vis[:, ch]  * std[ch] + mean[ch]\n",
    "        image_syn_vis[image_syn_vis<0] = 0.0\n",
    "        image_syn_vis[image_syn_vis>1] = 1.0\n",
    "        save_image(image_syn_vis, save_name, nrow=image_per_class) # Trying normalize = True/False may get better visual effects.\n",
    "\n",
    "\n",
    "    ''' Train synthetic data '''\n",
    "    net = get_network(channel, num_classes, im_size).to(device) # get a random model\n",
    "    net.train()\n",
    "    net_parameters = list(net.parameters())\n",
    "    optimizer_net = torch.optim.SGD(net.parameters(), lr=lr_net)  # optimizer_img for synthetic data\n",
    "    optimizer_net.zero_grad()\n",
    "    loss_avg = 0\n",
    "    dc_aug_param = None  # Mute the DC augmentation when learning synthetic data (in inner-loop epoch function) in oder to be consistent with DC paper.\n",
    "\n",
    "\n",
    "    for ol in range(outer_loop):\n",
    "\n",
    "        ''' freeze the running mu and sigma for BatchNorm layers '''\n",
    "        # Synthetic data batch, e.g. only 1 image/batch, is too small to obtain stable mu and sigma.\n",
    "        # So, we calculate and freeze mu and sigma for BatchNorm layer with real data batch ahead.\n",
    "        # This would make the training with BatchNorm layers easier.\n",
    "\n",
    "        BN_flag = False\n",
    "        BNSizePC = 16  # for batch normalization\n",
    "        for module in net.modules():\n",
    "            if 'BatchNorm' in module._get_name(): #BatchNorm\n",
    "                BN_flag = True\n",
    "        if BN_flag:\n",
    "            img_real = torch.cat([get_images(c, BNSizePC) for c in range(num_classes)], dim=0)\n",
    "            net.train() # for updating the mu, sigma of BatchNorm\n",
    "            output_real = net(img_real) # get running mu, sigma\n",
    "            for module in net.modules():\n",
    "                if 'BatchNorm' in module._get_name():  #BatchNorm\n",
    "                    module.eval() # fix mu and sigma of every BatchNorm layer\n",
    "\n",
    "\n",
    "        ''' update synthetic data '''\n",
    "        loss = torch.tensor(0.0).to(device)\n",
    "        for c in range(num_classes):\n",
    "            img_real = get_images(c, batch_test)\n",
    "            lab_real = torch.ones((img_real.shape[0],), device=device, dtype=torch.long) * c\n",
    "            img_syn = image_syn[c*image_per_class:(c+1)*image_per_class].reshape((image_per_class, channel, im_size[0], im_size[1]))\n",
    "            lab_syn = torch.ones((image_per_class,), device=device, dtype=torch.long) * c\n",
    "\n",
    "            if dsa:\n",
    "                seed = int(time.time() * 1000) % 100000\n",
    "                img_real = DiffAugment(img_real, dsa_strategy, seed=seed, param=dsa_param)\n",
    "                img_syn = DiffAugment(img_syn, dsa_strategy, seed=seed, param=dsa_param)\n",
    "\n",
    "            output_real = net(img_real)\n",
    "            loss_real = criterion(output_real, lab_real)\n",
    "            gw_real = torch.autograd.grad(loss_real, net_parameters)\n",
    "            gw_real = list((_.detach().clone() for _ in gw_real))\n",
    "\n",
    "            output_syn = net(img_syn)\n",
    "            loss_syn = criterion(output_syn, lab_syn)\n",
    "            gw_syn = torch.autograd.grad(loss_syn, net_parameters, create_graph=True)\n",
    "\n",
    "            loss += match_loss(gw_syn, gw_real, device, dis_metric)\n",
    "\n",
    "        optimizer_img.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_img.step()\n",
    "        loss_avg += loss.item()\n",
    "\n",
    "        if ol == outer_loop - 1:\n",
    "            break\n",
    "\n",
    "\n",
    "        ''' update network '''\n",
    "        image_syn_train, label_syn_train = copy.deepcopy(image_syn.detach()), copy.deepcopy(label_syn.detach())  # avoid any unaware modification\n",
    "        dst_syn_train = TensorDataset(image_syn_train, label_syn_train)\n",
    "        trainloader = torch.utils.data.DataLoader(dst_syn_train, batch_size=batch_train, shuffle=True, num_workers=0)\n",
    "        for il in range(inner_loop):\n",
    "            epoch('train', trainloader, net, optimizer_net, criterion, device, dsa, dsa_strategy, dsa_param, dc_aug_param, aug = True if dsa else False)\n",
    "\n",
    "\n",
    "    loss_avg /= (num_classes*outer_loop)\n",
    "\n",
    "    if it%10 == 0:\n",
    "        print('%s iter = %04d, loss = %.4f' % (get_time(), it, loss_avg))\n",
    "\n",
    "    if it == training_iteration: # only record the final results\n",
    "        data_save.append([copy.deepcopy(image_syn.detach().cpu()), copy.deepcopy(label_syn.detach().cpu())])\n",
    "        torch.save({'data': data_save, 'accs_all_exps': accs_all_exps, }, os.path.join(save_path, 'res_%s_%s_%s_%dipc.pt'%(method, 'FashionMNIST', 'ResNet34', image_per_class)))\n",
    "\n",
    "\n",
    "print('\\n==================== Final Results ====================\\n')\n",
    "for key in model_eval_pool:\n",
    "    accs = accs_all_exps[key]\n",
    "    print('Train on %s, evaluate %d random %s, mean  = %.2f%%  std = %.2f%%'%('ResNet34', len(accs), key, np.mean(accs)*100, np.std(accs)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

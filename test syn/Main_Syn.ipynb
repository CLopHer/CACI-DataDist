{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesis\n",
    "The main method of synthesis implemented here is \"Gradient Matching\" derived from [this](https://openreview.net/pdf?id=mSAKhLYLSsl) paper and code from [this](https://github.com/dm-medvedev/EfficientDistillation) repository. In a nutshell, the synthesized imageset is iteratively developed by taking sample minibatches of the training data, in this case Fashin MNIST, and computing the loss by applying a deep neural network. The associated batch of synthetic data with the samples is then updated by using gradient descent to match the change in loss with the training sample's. The parameters used on the deep neural network is updated to minimize loss after each iteration in which the synthetic data is updated. The end result is a fully synthesized data set. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "The primary resource used is PyTorch's 'torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "from utils import get_loops, get_dataset, get_network, get_eval_pool, evaluate_synset, get_daparam, match_loss, get_time, TensorDataset, epoch, DiffAugment, ParamDiffAug"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
